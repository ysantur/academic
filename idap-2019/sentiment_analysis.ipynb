{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/bulentsiyah/hepsi-burada-yorum veri seti kullanılmıştır\n",
    "\n",
    "# Kütüphaneler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Flatten, Dense, Dropout, GRU, Embedding\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Çalışma süresini ölçmek için\n",
    "start = time. time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setinin yüklenmesi\n",
    "training = pd.read_csv('hepsiburada.csv')\n",
    "\n",
    "Y = training['Rating'].values.tolist()  # 1:pozitif, 0:negatif\n",
    "X = training['Review'].values.tolist()  # Cümleler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Veri setini bölmek\n",
    "# %25 test, %75 eğitim için\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.25, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = 10000  # Külliyat içinde en sık geçen kaç kelime için tokenleştirme yapılacak\n",
    "tokenizer = Tokenizer(num_words=num_words) #num_words parametresi verilmezse tüm dataset kullanılır\n",
    "tokenizer.fit_on_texts(X) #Tokenleştirme yapılır, kelimeler integer'a dönüştürülür, en çok geçen en düşük integer'ı alır\n",
    "#tokenizer bir sözlüktür, key:kelimeler, value:tokenler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eğitim ve test verilerinin tokenleştirilmesi\n",
    "x_train_tokens = tokenizer.texts_to_sequences(x_train) \n",
    "x_test_tokens = tokenizer.texts_to_sequences(x_test)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Padding işlemi \n",
    "num_tokens = [len(tokens) for tokens in x_train_tokens + x_test_tokens]\n",
    "num_tokens = np.array(num_tokens)        \n",
    "max_tokens = np.mean(num_tokens) + 2 * np.std(num_tokens) # ortalama+2*std, 59 çıkacak\n",
    "max_tokens = int(max_tokens)\n",
    "np.sum(num_tokens < max_tokens) / len(num_tokens) # Yorumların %kaçı 59 dan az kelime içeriyor=%95.9\n",
    "x_train_pad = pad_sequences(x_train_tokens, maxlen=max_tokens)   #59dan küçükse 0 eklenir, büyükse atılır\n",
    "x_test_pad = pad_sequences(x_test_tokens, maxlen=max_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenlerden kelime elde etmek\n",
    "idx = tokenizer.word_index\n",
    "inverse_map = dict(zip(idx.values(), idx.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "182622/182622 [==============================] - 153s 837us/step - loss: 0.1749 - acc: 0.9522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x139cc00f0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Eğitim modeli\n",
    "model = Sequential()  \n",
    "embedding_size = 50   # Vektör sayısı, burada word2vec yada Glove algoritmalarıda kullanılabilir\n",
    "model.add(Embedding(input_dim=num_words,output_dim=embedding_size,input_length=max_tokens,name='embedding_layer'))\n",
    "\n",
    "# Cuda destekli GPU var ise GRU yerine CuDNNGRU kullanarak eğitim hızlandırabilir\n",
    "\n",
    "model.add(GRU(units=32, return_sequences=True, dropout=0.1)) \n",
    "model.add(GRU(units=16, return_sequences=True, dropout=0.1))  #GRU layer 16 output verecek, return_sequence=True ard arda output üretmek için\n",
    "model.add(GRU(units=8, return_sequences=True, dropout=0.1))   #GRU layer 8 output verecek\n",
    "model.add(GRU(units=4, dropout=0.1))                          #GRU layer 4 output verecek, return_sequence=False, çünkü bir sonraki layer=output\n",
    "model.add(Dense(1, activation='sigmoid'))        #Dense layer\n",
    "\n",
    "optimizer = Adam(lr=1e-3)\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimizer, metrics=['accuracy']) # İki sınıflı sınıflandırıcı\n",
    "model.fit(x_train_pad, y_train, epochs=1, batch_size=256) # 16 epoch yapıldığında doğruluk %99 üstüne çıkıyor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60875/60875 [==============================] - 68s 1ms/step\n",
      "32\n",
      "zaman: 1350.3059649467468\n"
     ]
    }
   ],
   "source": [
    "sonuclar = model.evaluate(x_test_pad, y_test) # sonuclar[1] Accuracy \n",
    "\n",
    "y_pred = model.predict(x=x_test_pad[0:1000])  #ilk 10000 veri için tahmin et\n",
    "y_pred = y_pred.T[0]                          #Sütun veriyi vektöre çevir\n",
    "\n",
    "cls_pred = np.array([1.0 if p>0.5 else 0.0 for p in y_pred])  #0.5 treshold, büyükse 1, küçükse 0\n",
    "cls_true = np.array(y_test[0:1000])                           #Gerçek sınıflar\n",
    "\n",
    "incorrect = np.where(cls_pred != cls_true)\n",
    "incorrect = incorrect[0]\n",
    "\n",
    "print(len(incorrect))\n",
    "\n",
    "idx = incorrect[0]    #ilk yanlış tahmin\n",
    "text = x_test[idx]    #bunun texti\n",
    "y_pred[idx]           #modelin tahmini\n",
    "cls_true[idx]         #gerçek sınıf\n",
    "\n",
    "end = time. time()\n",
    "\n",
    "print(\"zaman:\", end-start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.9840717]\n",
      " [0.3818984]]\n"
     ]
    }
   ],
   "source": [
    "# Modeli metin veri ile denemek\n",
    "tst=[\"ürün kargodan zamanında geldi 6 aydır sorunsuz kullanıyorum\", \"tek kelime ile berbat hiç beklediğim gibi değil\"]\n",
    "tokens = tokenizer.texts_to_sequences(tst)\n",
    "tokens_pad = pad_sequences(tokens, maxlen=max_tokens)\n",
    "tokens_pad.shape\n",
    "print(model.predict(tokens_pad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
